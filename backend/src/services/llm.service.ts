import { Injectable, Logger } from '@nestjs/common';
import OpenAI from 'openai';
import { GoogleGenerativeAI } from '@google/generative-ai';

@Injectable()
export class LlmService {
  private readonly logger = new Logger(LlmService.name);
  private openai: OpenAI | null = null;
  private gemini: GoogleGenerativeAI | null = null;
  private provider: string;

  constructor() {
    this.provider = process.env.LLM_PROVIDER || 'openai';

    if (this.provider === 'openai' && process.env.OPENAI_API_KEY) {
      this.openai = new OpenAI({
        apiKey: process.env.OPENAI_API_KEY,
      });
    } else if (this.provider === 'gemini' && process.env.GEMINI_API_KEY) {
      this.gemini = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
    }
  }

  private async withRetry<T>(operation: () => Promise<T>, retries = 3, delay = 2000): Promise<T> {
    try {
      return await operation();
    } catch (error: any) {
      if (retries > 0 && (error.status === 429 || error.message?.includes('429') || error.message?.includes('Quota exceeded'))) {
        this.logger.warn(`Rate limit hit. Retrying in ${delay}ms... (${retries} retries left)`);
        await new Promise((resolve) => setTimeout(resolve, delay));
        return this.withRetry(operation, retries - 1, delay * 2);
      }
      throw error;
    }
  }

  async generateEmbedding(text: string): Promise<number[]> {
    try {
      return await this.withRetry(async () => {
        if (this.provider === 'openai' && this.openai) {
          const response = await this.openai.embeddings.create({
            model: process.env.EMBEDDING_MODEL || 'text-embedding-3-small',
            input: text,
          });
          return response.data[0].embedding;
        } else if (this.provider === 'gemini' && this.gemini) {
          const model = this.gemini.getGenerativeModel({ model: 'text-embedding-004' });
          const result = await model.embedContent(text);
          return result.embedding.values;
        }
        throw new Error('No LLM provider configured');
      });
    } catch (error) {
      this.logger.error(`Error generating embedding: ${error.message}`);
      throw error;
    }
  }

  async extractRequirements(jobDescription: string): Promise<string[]> {
    const prompt = `Extract the key skills, technologies, and requirements from this job description. Return only a JSON array of strings, no other text.

Job Description:
${jobDescription}

Return format: ["skill1", "skill2", "skill3"]`;

    try {
      return await this.withRetry(async () => {
        if (this.provider === 'openai' && this.openai) {
          const response = await this.openai.chat.completions.create({
            model: 'gpt-4-turbo-preview',
            messages: [
              {
                role: 'system',
                content: 'You are a helpful assistant that extracts job requirements. Always return valid JSON arrays only.',
              },
              { role: 'user', content: prompt },
            ],
            temperature: 0.3,
          });

          const content = response.choices[0].message.content;
          const cleaned = content?.trim().replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();
          return JSON.parse(cleaned || '[]');
        } else if (this.provider === 'gemini' && this.gemini) {
          // Use gemini-1.5-flash which is more standard/stable
          const model = this.gemini.getGenerativeModel({ model: 'gemini-1.5-flash' });
          const result = await model.generateContent(prompt);
          const response = result.response.text();
          const cleaned = response.trim().replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();
          return JSON.parse(cleaned || '[]');
        }
        throw new Error('No LLM provider configured');
      });
    } catch (error) {
      this.logger.error(`Error extracting requirements: ${error.message}`);
      // Fallback: return empty array
      return [];
    }
  }

  async tailorResume(
    masterResume: string,
    jobDescription: string,
    requirements: string[],
  ): Promise<string> {
    const prompt = `You are an expert resume writer specializing in ATS (Applicant Tracking System) optimization.

Given the user's master resume and a job description, create a tailored resume that:
1. Highlights relevant skills and experiences from the master resume
2. Uses keywords from the job requirements
3. Maintains truthfulness (only include what's in the master resume)
4. Is formatted for ATS systems (plain text, clear sections)

Master Resume:
${masterResume}

Job Description:
${jobDescription}

Key Requirements:
${requirements.join(', ')}

Return the tailored resume as plain text, optimized for this specific job.`;

    try {
      return await this.withRetry(async () => {
        if (this.provider === 'openai' && this.openai) {
          const response = await this.openai.chat.completions.create({
            model: 'gpt-4-turbo-preview',
            messages: [
              {
                role: 'system',
                content: 'You are an expert resume writer. Return only the tailored resume text, no explanations.',
              },
              { role: 'user', content: prompt },
            ],
            temperature: 0.7,
          });

          return response.choices[0].message.content || masterResume;
        } else if (this.provider === 'gemini' && this.gemini) {
          const model = this.gemini.getGenerativeModel({ model: 'gemini-1.5-flash' });
          const result = await model.generateContent(prompt);
          return result.response.text();
        }
        throw new Error('No LLM provider configured');
      });
    } catch (error) {
      this.logger.error(`Error tailoring resume: ${error.message}`);
      // Fallback: return original resume
      return masterResume;
    }
  }

  async answerApplicationQuestions(
    questions: { id: string; label: string; type: string }[],
    userProfile: any,
    resumeText: string,
    jobDescription?: string,
  ): Promise<Record<string, string>> {
    const prompt = `
You are a smart job application assistant.
Your task is to generate answers for the following job application form fields based on the user's profile and resume.

User Profile: ${JSON.stringify(userProfile)}
Resume Summary: ${resumeText.substring(0, 3000)}...
Job Description Context: ${jobDescription ? jobDescription.substring(0, 1000) : 'Not provided'}

Form Fields to Fill:
${JSON.stringify(questions, null, 2)}

Instructions:
1. For each field, provide the exact text to fill.
2. If the field is a "cover_letter" or "why_hired", write a compelling, professional, and personalized paragraph (3-5 sentences) connecting the user's skills to the job.
3. If asking for "Current Verification" or "Location", use the user's location.
4. If asking for "Availability", answer "Immediately" or "As per notice period" (assume immediately if not specified).
5. For numeric fields (experience, salary), provide just the number or range.
6. Return ONLY a JSON object mapping 'id' to 'answer'.
Example format: { "field_id_1": "Answer text", "field_id_2": "12" }
`;

    try {
      return await this.withRetry(async () => {
        let responseText = '';
        if (this.provider === 'openai' && this.openai) {
          const response = await this.openai.chat.completions.create({
            model: 'gpt-4-turbo-preview',
            messages: [{ role: 'system', content: 'You are a helpful assistant. Return generic JSON only.' }, { role: 'user', content: prompt }],
            response_format: { type: 'json_object' },
            temperature: 0.7,
          });
          responseText = response.choices[0].message.content || '{}';
        } else if (this.provider === 'gemini' && this.gemini) {
          // Gemini handling
          const model = this.gemini.getGenerativeModel({ model: 'gemini-1.5-flash', generationConfig: { responseMimeType: "application/json" } as any });
          const result = await model.generateContent(prompt);
          responseText = result.response.text();
        } else {
          // Fallback for OpenAI Compatible (Groq) via OpenAI SDK if configured as 'openai' but pointing to Groq
          // Assuming existing OpenAI fallback works if ENV vars are set for Groq
          throw new Error('LLM Provider not configured for generic chat');
        }

        // Clean and parse
        const cleaned = responseText.trim().replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();
        return JSON.parse(cleaned);
      });
    } catch (error) {
      this.logger.error(`Error answering questions: ${error.message}`);
      // Fallback: simple mapping
      const fallback: Record<string, string> = {};
      questions.forEach(q => {
        if (q.label.toLowerCase().includes('name')) fallback[q.id] = userProfile.name;
        else if (q.label.toLowerCase().includes('email')) fallback[q.id] = userProfile.email;
        else fallback[q.id] = "Please refer to my resume.";
      });
      return fallback;
    }
  }
}



